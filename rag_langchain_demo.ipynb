{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7048cb1b-e299-4b87-a646-96b77364943c",
   "metadata": {},
   "source": [
    "# Retrieval Augmented Generation Pyhton Demo #\n",
    "\n",
    "The purpose of this demo is to show my lessons learned about RAG and to show my skills in pyhton.\n",
    "\n",
    "This Demo uses simple local components to build a fully functional Retrieval-Augmented Generation (RAG) system without requiring cloud APIs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582f6fec-22ae-4a1b-b6e1-c046aea3e0be",
   "metadata": {},
   "source": [
    "## Code: ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e2797303-1cd0-4cd3-a530-306fc5ff5d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This model turns text chunks into vector embeddings for similarity search\n",
    "\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "embedding = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a7dfdc04-6649-4f5b-aba1-75e85c984761",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Breaking the document into overlapping chunks that fit inside the LLM's token limit\n",
    "\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "# Loading the Wikipedia page text and splitting it\n",
    "loader = TextLoader(\"example.txt\")\n",
    "documents = loader.load()\n",
    "text_splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "docs = text_splitter.split_documents(documents)\n",
    "\n",
    "# Local embeddings\n",
    "embedding = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# FAISS vectorstore allows fast similarity-based retrieval later\n",
    "vectorstore = FAISS.from_documents(docs, embedding)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "29eac379-51f2-4bce-8728-be5d1706f280",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading flan-t5-base...\n",
      "Model ready.\n"
     ]
    }
   ],
   "source": [
    "# Using flan-t5-base for efficient generation which needs no API key unlike OpenAI which I had problems with a little\n",
    "\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_huggingface import HuggingFacePipeline\n",
    "from transformers import pipeline\n",
    "\n",
    "print(\"Downloading flan-t5-base...\")\n",
    "llm_pipeline = pipeline(\"text2text-generation\", model=\"google/flan-t5-base\", max_length=512)\n",
    "print(\"Model ready.\")\n",
    "\n",
    "# Wraping HuggingFace pipeline in LangChain's interface\n",
    "llm = HuggingFacePipeline(pipeline=llm_pipeline)\n",
    "\n",
    "# Connecting the retriever + LLM to perform context-aware answers\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2ec0b813-e2f3-4200-ab5e-a2b42fb90d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a simple ask function for questions\n",
    "def ask(query: str):\n",
    "    result = qa_chain({\"query\": query})\n",
    "    print(\"Answer:\", result[\"result\"])\n",
    "    \n",
    "# Uncomment the following lines to get sources\n",
    "    #for i, doc in enumerate(result[\"source_documents\"], 1):\n",
    "        #print(f\"\\n--- Source {i} ---\\n{doc.page_content[:500]}...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd93dc9-b244-4daf-bca0-43060aa69bd2",
   "metadata": {},
   "source": [
    "## Now some simple questions to test the RAG ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4eedf109-bfb3-4dd2-aeb0-3065e2b28898",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: Norway\n"
     ]
    }
   ],
   "source": [
    "ask(\"Where is Kongsberg from?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "bfa05716-5caf-43b6-9def-d043cedc098d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: Geir Hy\n"
     ]
    }
   ],
   "source": [
    "ask(\"Who is the CEO of Kongsberg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b7343d80-8ad9-4f8d-b541-ab01ee497ba9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: Kongsberg Maritime Kongsberg Defence & Aerospace Kongsberg Discovery Kongsberg Digital\n"
     ]
    }
   ],
   "source": [
    "ask(\"What 4 braches are there to Kongsberg?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90310dd5-ea9a-4ee9-badf-8c6736d82434",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
